<h1 align="center">
RICO: An Enhanced Image Recaptioning Method via Visual Reconstruction
</h1>


<br>

This is the repo for the official implementation of the paper: [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/)

## üìÜ TODO List
- [x] Code for the full RICO pipeline.
- [ ] Pretrained checkpoint for RICO-Flash.
- [ ] Training method for your own DPO-based model.

## üí° Introduction

<div align=center>
<img src="img/method.png" width="800" >
</div>


Existing recaptioning methods typically rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions, but often suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details. 

To address these issues, **we propose RICO, a novel framework that enhances captions through an iterative visual reconstruction-and-refinement pipeline**. Our key idea is to:

1. Reconstruct the caption into an image using a text-to-image model.

2. Compare the original image with the reconstructed image using an MLLM.

3. Refine the caption based on detected discrepancies.

This iterative process leads to more accurate and comprehensive captions.
To further reduce the computational cost of multiple iterations, we introduce RICO-Flash, a lightweight variant trained with Direct Preference Optimization (DPO) to emulate RICO‚Äôs behavior in a single step.







## ‚öôÔ∏è Installation & Setup
### Environment Setup

Required packages and dependencies are listed in the `requirements.txt` file. You can install the environment using Conda with the following command:

```bash
conda env create -n rico python=3.11
conda activate rico
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```

### Add Your GPT-4o Integration
To use GPT-4o in the caption refinement process, you need to implement your own API call method in `models_util/gpt4o.py`. The current implementation is a placeholder and should be replaced with your actual API logic:

```python
def call_gpt4o(orig_img_path, new_img_path, prompt):
    pass
    #! Please implement the function to call GPT-4o with the provided prompt and return the response.
    #! This function should handle the API call to GPT-4o, passing the original and reconstructed image paths along with the prompt.
    #! The function should return the revised caption generated by GPT-4o.
```

### Plug-and-Play Design
RICO is designed with a modular and extensible architecture, making it easy to plug in your own models for various components. Specifically, you can:

- Replace the text-to-image model by modifying `models_util/flux.py`.

- Swap out the initial captioning model in `models_util/qwen_single.py`.

- Integrate your own caption refinement model by editing `models_util/gpt4o.py`.

Simply follow the existing interfaces and structures defined in these files to ensure compatibility.


## üöÄ Usage
To run the RICO pipeline, you can use the provided `main_loop.py` script. 

We list some important arguments below, you can also check the full list of arguments in `main_loop.py`:


```python

# define the number of iterations for the reconstruction-refinement process
ITER_STEPS = 2

# define the path to the image folder containing the images to be processed
parser.add_argument('--image_folder', type=str, default='datasets/capsbench')

# if you have pre-generated captions, you can provide the path to the caption JSON file
parser.add_argument('--caption_json', type=str, default=None)

# define the path to the output directory for saving iterative images
parser.add_argument('--output_video_dir', type=str, default='results/outputs')

# define the path to the output directory for saving iterative captions
parser.add_argument('--output_json_dir', type=str, default='results/records')

```




## ‚òï Citation
 If you find our projects helpful to your research, please consider citing our paper:
```

```
For any issues or further discussions, feel free to contact wangyuchi369@gmail.com

